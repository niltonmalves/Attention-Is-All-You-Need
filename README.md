# Attention-Is-All-You-Need
Attention Is All You Need explained
$$
\frac{1}{c} \cdot \int_{1}^8 x^2 dx 
$$

$$
J_{1}^{V}=\left\{W_{1} \in \Omega:\right. \text{Jogador somar 7 pontos}\} \text{ 6 pontos de amostra}
$$
Attention Is All You Need

https://www.youtube.com/watch?v=iDulhoQ2pro

CS480/680 Lecture 19: Attention and Transformer Networks

https://www.youtube.com/watch?v=OyFJWRnt_AY

https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634

http://peterbloem.nl/blog/transformers



Transformer Neural Networks - EXPLAINED! (Attention is all you need)

https://www.youtube.com/watch?v=TQQlZhbC5ps

ver sobre :

- hidden representation

- softmax function

- wordvector

